## 数据分析
主要使用包sklearn

常用算法简单说明如下


### 决策树 
做分类 回归
选择哪个属性作为根节点；
选择哪些属性作为子节点；
什么时候停止并得到目标状态，即叶节点。

预测泰坦尼克生存率
#### ID3 
基于信息增益节点属性

#### c4.5 
基于信息增益率节点属性 
#### Cart


### 朴素贝叶斯
朴素贝叶斯分类常用于文本分类，尤其是对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。
##### 相关概念：
先验概率
后验概率
条件概率
似然函数（likelihood function）

数据分为 ，一种是离散数据，另一种是连续数据

离散数据
身高“高”、体重“中”，鞋码“中”，请问这个人是男还是女？


连续数据案例
那么如果给你一个新的数据，身高 180、体重 120，鞋码 41，请问该人是男是女呢？
由于身高、体重、鞋码都是连续变量数字区间

### svm支持向量机 
模式识别、分类、回归分析
文本分类、图像识别
### knn
分类 和 回归

1数据加载：我们可以直接从 sklearn 中加载自带的手写数字数据集；                    
2准备阶段：在这个阶段中，我们需要对数据集有个初步的了解，比如样本的个数、图像长什么样、识别结果是怎样的。你可以通过可视化的方式来查看图像的呈现。通过数据规范化可以让数据都在同一个数量级的维度。另外，因为训练集是图像，每幅图像是个 8*8 的矩阵，我们不需要对它进行特征选择，将全部的图像数据作为特征值矩阵即可；                                                        
3分类阶段：通过训练可以得到分类器，然后用测试集进行准确率的计算。


### k-mean
解决聚类问题 k 类中心点

## EM

 案例 王者荣耀 归类英雄


1首先我们需要加载数据源；

2在准备阶段，我们需要对数据进行探索，包括采用数据可视化技术，让我们对英雄属性以及这些属性之间的关系理解更加深刻，然后对数据质量进行评估，是否进行数据清洗，最后进行特征选择方便后续的聚类算法；

3聚类阶段：选择适合的聚类模型，这里我们采用 GMM 高斯混合模型进行聚类，并输出聚类结果，对结果进行分析。

## 关联规则挖掘
Apriori 是一种挖掘关联规则（association rules）的算法，它通过挖掘频繁项集（frequent item sets）来揭示物品之间的关联关系，被广泛应用到商业挖掘和网络安全等领域中。频繁项集是指经常出现在一起的物品的集合，关联规则暗示着两种物品之间可能存在很强的关系
### 概念

支持度 

    指的是某个商品组合出现的次数与总次数之间的比例

置信度

    就是说在 A 发生的情况下，B 发生的概率是多少

提升度

    商品 A 的出现，对商品 B 的出现概率提升的”程度。 提升度 (A→B)= 置信度 (A→B)/ 支持度 (B)

    提升度 (A→B)>1：代表有提升；

    提升度 (A→B)=1：代表有没有提升，也没有下降；

    提升度 (A→B)<1：代表有下降。

### Apriori流程
K=1，计算 K 项集的支持度；

筛选掉小于最小支持度的项集；

如果项集为空，则对应 K-1 项集的结果为最终结果。

### FP-Growth 
Apriori算法的改进

常用 FP-Growth 来做频繁项集的挖掘

FP-Growth 算法，它的特点：

创建了一棵 FP 树来存储频繁项集。在创建前对不满足最小支持度的项进行删除，减少了存储空间。我稍后会讲解如何构造一棵 FP 树；整个生成过程只遍历数据集 2 次，大大减少了计算量。

FP-Growth 的原理
图解fp树构建 https://www.cnblogs.com/pinard/p/6307064.html
    1 创建项头表
    2 构造fp树
    3 通过fp树挖掘频繁项集
## adaboost 
分类 回归分析
在数据挖掘中，分类算法可以说是核心算法，其中 AdaBoost 算法与随机森林算法一样都属于分类算法中的集成算法。
通过训练不同的弱分类器，将这些弱分类器集成起来形成一个强分类器。在每一轮的训练中都会加入一个新的弱分类器，直到达到足够低的错误率或者达到指定的最大迭代次数为止。实际上每一次迭代都会引入一个新的弱分类器（这个分类器是每一次迭代中计算出来的，是新的分类器，不是事先准备好的）。

## 随机森林
随机森林的英文是 Random Forest，英文简写是 RF。它实际上是一个包含多个决策树的分类器，每一个子分类器都是一棵 CART 分类回归树。所以随机森林既可以做分类，又可以做回归。当它做分类的时候，输出结果是每个子分类器的分类结果中最多的那个。你可以理解是每个分类器都做投票，取投票最多的那个结果。当它做回归的时候，输出结果是每棵 CART 树的回归结果的平均值。

```

# -*- coding: utf-8 -*-
# 使用RandomForest对IRIS数据集进行分类
# 利用GridSearchCV寻找最优参数,使用Pipeline进行流水作业
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
rf = RandomForestClassifier()
parameters = {"randomforestclassifier__n_estimators": range(1,11)}
iris = load_iris()
pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('randomforestclassifier', rf)
])
# 使用GridSearchCV进行参数调优
clf = GridSearchCV(estimator=pipeline, param_grid=parameters)
# 对iris数据集进行分类
clf.fit(iris.data, iris.target)
print("最优分数： %.4lf" %clf.best_score_)
print("最优参数：", clf.best_params_)
运行结果：
最优分数： 0.9667
最优参数： {'randomforestclassifier__n_estimators': 9}
```
#### 案例分析 信用卡违约率

流程

1.加载数据；

2.准备阶段：探索数据，采用数据可视化方式可以让我们对数据有更直观的了解，比如我们想要了解信用卡违约率和不违约率的人数。因为数据集没有专门的测试集，我们还需要使用 train_test_split 划分数据集。

3.分类阶段：之所以把数据规范化放到这个阶段，是因为我们可以使用 Pipeline 管道机制，将数据规范化设置为第一步，分类为第二步。因为我们不知道采用哪个分类器效果好，所以我们需要多用几个分类器，比如 SVM、决策树、随机森林和 KNN。然后通过 GridSearchCV 工具，找到每个分类器的最优参数和最优分数，最终找到最适合这个项目的分类器和该分类器的参数。


代码文件 analysis_pro/credit_card_demo_1.py

## 逻辑回归
也叫作 logistic 回归。虽然名字中带有“回归”，但它实际上是分类方法，主要解决的是二分类问题，当然它也可以解决多分类问题，只是二分类更常见一些
在逻辑回归中使用了 Logistic 函数，也称为 Sigmoid 函数
项目流程
	数据探索、数据规范化、数据集划分
	模型创建、模型训练、模型评估

## 模型评估指标
TP：预测为正，判断正确；

FP：预测为正，判断错误；

TN：预测为负，判断正确；

FN：预测为负，判断错误。

F1 作为精确率 P 和召回率 R 的调和平均，数值越大代表模型的结果越好。


## pageRank

